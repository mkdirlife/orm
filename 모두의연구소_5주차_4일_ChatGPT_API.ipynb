{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeLQV5akv6oa0Yn9kZw1SH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkdirlife/orm/blob/main/%EB%AA%A8%EB%91%90%EC%9D%98%EC%97%B0%EA%B5%AC%EC%86%8C_5%EC%A3%BC%EC%B0%A8_4%EC%9D%BC_ChatGPT_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9sp64FUCuqQ",
        "outputId": "16085a39-e28c-45b1-d76d-2563dcee4c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.9.0 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI  # OpenAI 모듈 불러오기\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "client = OpenAI(api_key='키값키값')  # 사용자의 API 키로 대체해야 함\n",
        "\n",
        "# ChatGPT를 사용한 텍스트 생성 요청\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\" : \"user\", \"content\" : \"Hello World!\"}]\n",
        ")\n",
        "# API 응답에서 마지막 메시지의 내용을 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "x2JHz9lrC8e7",
        "outputId": "3de82760-a8fe-4c96-d5db-68be975017c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2bf598492de4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ChatGPT를 사용한 텍스트 생성 요청\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"Hello World!\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 648\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[0;32m-> 1179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 868\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    869\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    945\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    945\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# ChatGPT를 사용한 텍스트 생성 요청\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\" : \"user\", \"content\" : \"Translate the following English text to korean: 'Hello, how are you?'\"}]\n",
        ")\n",
        "\n",
        "#응답 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxDIqPptERSX",
        "outputId": "6e953506-76d7-4fe7-8aa9-5a848a6370ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요, 어떻게 지내세요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-3.5-turbo\",\n",
        "        messages = [{\"role\" : \"user\", \"content\" : \"Translate the following korean text to English: 안녕하세요 오늘 날씨가 좋네요. 햇살이 맑아요\"}],\n",
        "        max_tokens=10,\n",
        "        temperature=0.9,\n",
        "        top_p=0.8,\n",
        "        frequency_penalty=0.2\n",
        "    )\n",
        "    print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8WfFqSHPT86",
        "outputId": "0166a8b5-5b18-4a57-a4ce-86dd2ddfc10b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, today's weather is good. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunshine\n",
            "Hello, today's weather is nice. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_chatbot(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-3.5-turbo\", messages = messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "prompt_role = \"너는 블로그 전문가, 파워블로그처럼 글을 써야해.\\\n",
        "                개발자의 직업관에 대한 글을 써야하고,\\\n",
        "                그리고 취업을 준비하는 20대 독자들에게 잘 보일수 있도록 글을 써야되\\\n",
        "                SEO최적화된 글을 써야되\""
      ],
      "metadata": {
        "id": "PhM2PJPVQMEb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def assist_blogger(\n",
        "    facts: List[str], tone: str, length_words: int, style: str\n",
        "):\n",
        "    facts = \", \".join(facts)\n",
        "    prompt_role = \"너는 블로그 전문가고, 파워블로그처럼 글을 써야해\"\n",
        "    prompt = f\"{prompt_role} \\\n",
        "            FACTS: {facts} \\\n",
        "            TONE: {tone} \\\n",
        "            LEGNTH: {length_words} words \\\n",
        "            STYLE: {style}\"\n",
        "    return ask_chatbot([{\"role\": \"user\", \"content\": prompt}])"
      ],
      "metadata": {
        "id": "OMtfy8F4Qop1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    assist_blogger(\n",
        "        [\"대학 진학 이후의 개발자의 삶은?\"], \"informal\", 100, \"blogpost\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQSe7ASKQ05u",
        "outputId": "85b73669-e2e4-484b-d829-72028d0d0394"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어머나! 블로그 전문가로서, 오늘은 대학 진학 이후의 개발자의 삶에 대해 이야기해볼까 해요. 사실 이 부분은 많은 사람들이 궁금해하는 주제인데요.\n",
            "\n",
            "대학을 졸업하고 나서 개발자로 일하게 되면 생활은 어떻게 변할까요? 일단, 저는 개발자로 일하기 시작하면 돈을 좀 더 벌게 될 것이라고 기대하시면 됩니다. 개발자의 시급은 비교적 높기 때문에, 노력에 비례하여 보람을 느낄 수 있을 거예요.\n",
            "\n",
            "하지만, 개발자로서의 삶은 쉬운 길만은 아니에요. 서류 작업, 끊이지 않는 프로젝트들... 그리고 무한한 공부! 대학시절과 달리, 자기 계발을 위한 시간이 없어지기도 하고요. 하지만, 어떤 일을 하든 어려움은 따르는 법이죠. 그러한 어려움을 극복하면서 성장하는 과정에서 개발자로서의 지식과 경험이 쌓여갈 것이라 생각해요.\n",
            "\n",
            "그리고, 개발자로서 성공하기 위해서는 새로운 기술과 트렌드에 대한 열린 마음이 필요해요. IT 산업은 빠르게 변화하기 때문에, 시대에 뒤쳐지지 않도록 끊임없이 공부하고 발전해야 합니다. 이것이야말로 개발자로서의 삶의 한 부분이에요.\n",
            "\n",
            "사실 대학 진학 이후의 개발자의 삶은 잠재력이 다양하게 펼쳐질 수 있는 시기이기도 해요. 자신의 관심 분야를 찾고 그에 따라 스킬을 발전시키면, 독특하고 흥미로운 프로젝트에도 도전할 수 있어요. 그리고, 개발자로서 성장하면서 스스로 소중한 인맥들을 만들어갈 수도 있어요. 혼자 일하는 개발자도 있지만, 협업은 언제나 좋은 선택이죠.\n",
            "\n",
            "대학 진학 이후의 개발자의 삶은 막막하고 힘들기도 하지만, 또한 흥미로운 도전들과 소중한 성장의 기회로 가득찬 시기입니다. 열정과 끈기를 가지고, 새로운 도전을 두려워하지 않는 마음으로 개발자로서 성장하실 수 있을 거예요. 막막해 보이는 그 길을 당당하게 걷는 모습을 기대합니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    assist_blogger(\n",
        "        facts =[\"Chat GPT 등장이후의 취업 전략은?\"],\n",
        "        tone = \"정중하게\",\n",
        "        length_words = 200,\n",
        "        style= \"파워블로그 스럽게\"\n",
        "        )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M94FTIV1RGZ_",
        "outputId": "7392e6a1-49b4-42b7-eb70-8c7c62f06942"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요! 저는 블로그 전문가로서 여러분에게 취업 전략에 관한 최신 정보를 제공해드리려고 합니다. 특히 Chat GPT와 같은 인공지능 기술의 등장 이후에는 어떻게 취업에 대비해야 하는지 알아보도록 하겠습니다.\n",
            "\n",
            "Chat GPT는 최근에 등장한 자연어 처리 모델로, 많은 기업들이 채팅 봇이나 가상 비서 등에서 활용하고 있습니다. 따라서 이러한 기술의 발전은 채용 과정에서도 중요한 영향을 미치고 있습니다.\n",
            "\n",
            "첫째로, Chat GPT를 활용하여 이력서와 자기소개서를 작성해보세요. 이러한 자동화된 글쓰기 기술은 효율적인 이력서 작성을 가능하게 해주므로, 경쟁력 있는 문장을 구성할 수 있습니다.\n",
            "\n",
            "둘째로, Chat GPT 기술을 이용하여 인터뷰를 준비하세요. 이 모델은 다양한 질문에 대해 실시간으로 답변을 생성할 수 있는 능력을 가지고 있으므로, 자주 나오는 질문에 대해 미리 연습해두는 것이 좋습니다.\n",
            "\n",
            "마지막으로, 소셜 미디어를 적극 활용하세요. Chat GPT는 대화를 통해 사람들과 자연스럽게 소통할 수 있는 능력을 갖추고 있습니다. 블로그나 트위터와 같은 플랫폼을 통해 자신의 전문성을 공유하고, 산업 동향에 대해 논의를 제안해보세요. 이를 통해 기업들의 주목을 받을 수 있을 것입니다.\n",
            "\n",
            "위와 같은 전략을 통해 취업 경쟁에서 우위를 점할 수 있습니다. Chat GPT와 같은 인공지능 기술의 발전에 뒤쳐지지 않고, 적극적으로 활용하는 것이 중요합니다. 이를 통해 당신의 취업 전략에 새로운 힘과 활력을 더해보세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첨부된 파일을 읽고 작은 부분으로 나누기 위한 코드입니다.\n",
        "\n",
        "# 파일 경로 설정\n",
        "file_path = '/content/English_But_what_is_a_neural_network____Chapter_1_Deep_learning_DownSub.com.txt'\n",
        "\n",
        "# 파일을 읽어서 내용을 저장\n",
        "with open(file_path, 'r') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# 텍스트를 나눌 최대 길이 설정 (토큰 수가 아닌 문자 수 기준)\n",
        "max_length = 5000  # 각 부분의 최대 길이 (문자 수)\n",
        "\n",
        "# 텍스트를 작은 부분으로 나누는 함수\n",
        "def split_into_parts(text, length):\n",
        "    return [text[i:i+length] for i in range(0, len(text), length)]\n",
        "\n",
        "# 텍스트를 여러 부분으로 나눔\n",
        "parts = split_into_parts(transcript, max_length)\n",
        "\n",
        "# 나누어진 부분들의 수와 첫 부분의 내용 일부를 출력\n",
        "num_parts = len(parts)\n",
        "first_part_preview = parts[0][:500]  # 첫 부분의 처음 500자\n",
        "\n",
        "num_parts, first_part_preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoeHFIWdTMa2",
        "outputId": "8f77a28a-e647-4c05-ad35-5d1b6a4c745a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " \"This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,\\n\\nbut your brain has no trouble recognizing it as a 3. And I want you to take a moment\\n\\nto appreciate how crazy it is that brains can do this so effortlessly. I mean, this,\\n\\nthis and this are also recognizable as 3s, even though the specific values of each pixel\\n\\nis very different from one image to the next. The particular light-sensitive cells in your\\n\\neye that are firing when you see this 3 are very \")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 부분만 사용\n",
        "first_part = parts[0]\n",
        "\n",
        "# 첫 번째 부분에 대한 번역 요청\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \"너는 유튜브를 영어에서 한국어로 번역하는 번역가이자, 요약을 잘하는 역할을 할꺼야\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"업로드한 파일을 한국어로 번역해줘\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"한국어로 번역한 내용을 요약해\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : first_part}\n",
        "      ],\n",
        ")\n",
        "\n",
        "# 번역 결과 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-QRm1GVUYvI",
        "outputId": "6156890d-df38-4f26-f024-f831f23e79a9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이것은 한글 3입니다. 그것은 서투르게 작성되었으며 극도로 낮은 해상도인 28x28 픽셀로 렌더링되었습니다. 그러나 당신의 뇌는 여전히 이것을 3으로 인식하는 데 어려움을 겪지 않습니다. 이것은 얼마나 놀랍게도 뇌가 이를 손쉽게 수행할 수 있다는 것을 감상하는 시간을 가질 것입니다.\n",
            "\n",
            "개별 픽셀의 특정 값이 서로 다른 이미지에서 다르다고 하더라도, 이것, 이것, 그리고 이것도 3으로 인식됩니다. 눈의 특정 민감한 세포들이 이 3을 보는 것과 이 3을 보는 것이 다릅니다. 그러나 당신의 믿을 수 없을만큼 똑똑한 시각 피질에서는 이를 동일한 개념으로 해석하면서도 다른 이미지를 별도의 개념으로 인식합니다.\n",
            "\n",
            "하지만 28x28 그리드를 입력받고 0에서 10 사이의 하나의 숫자를 출력하는 프로그램을 작성하라는 것이라면, 이 과제는 웃기게도 아주 어렵습니다. 돌 뒤에 살고 있다면, 머신러닝과 신경망의 관련성과 중요성을 설명할 필요가 없을 것입니다. 그러나 여기에서 할 일은 신경망이 무엇인지, 배경이 없다고 가정하고 시각적으로 그것이 무엇을 하는지, 허프맞춤법으로 하지 말고 수학의 일부로서 이해할 수 있도록 보여주는 것입니다. 저의 희망은 구조 자체가 동기를 부여받았음을 느끼게하고, 뉴럴 네트워크가 '배우고 있다'는 것을 읽거나 들을 때 무슨 의미인지 알 수 있다고 느끼는 것입니다.\n",
            "\n",
            "우리는 손으로 쓴 숫자를 인식하는 데 학습할 수 있는 신경망을 구성할 것입니다. 이는 주제를 소개하는데 어느 정도 고전적인 예입니다. 이러한 상태 유지를 선택하는 것은 원하는 바에 대해 설득력을 부여하기 때문에 기존 상태로 머무릅니다. 2개의 비디오의 끝에는 더 많이 배울 수 있는 좋은 자료와 코드를 다운로드하여 컴퓨터에서 직접 실험해볼 수 있는 장소를 알려드리고 싶습니다.\n",
            "\n",
            "뉴럴 네트워크의 변형은 많이 있으며, 최근 몇 년동안 이러한 변형에 대한 연구가 급격히 증가했습니다. 그러나 이 두 가지 입문 비디오에서는 우리는 추가적인 장식이 없는 가장 간단한 형태의 신경망만 살펴볼 것입니다. 이 복잡도는 이해하기에 충분합니다. 단순한 형태에서도 손으로 쓴 숫자를 인식할 수 있으며, 컴퓨터가 할 수 있는 멋진 일입니다. 동시에 우리가 기대하는 몇 가지 희망에는 미달하게 됩니다.\n",
            "\n",
            "네트워크는 뇌를 모델로 하지만 뇌는 어떻게 동작하는지 알아보겠습니다. 뉴런이 무엇이고 어떤 의미에서 서로 연결되어 있는지 알아봅시다. 현재 나는 뉴런이라고 말할 때 당신이 할 일은 0에서 1 사이의 수를 가지고 있는 것을 생각하는 것입니다. 그 이상도 그 이하도 아니에요. 예를 들어, 네트워크는 입력 이미지의 각 픽셀에 해당하는 많은 뉴런으로 시작합니다. 전체적으로 784개의 뉴런이 있습니다. 각각은 해당 픽셀의 그레이스케일 값을 나타내는 수를 가지고 있는데, 검은색 픽셀에는 0, 흰색 픽셀에는 1까지 범위가 있습니다. 이 뉴런 안의 수를 활성화라고 하며, 개별 뉴런이 높은 숫자일 때 켜진 것처럼 생각할 수 있습니다. 이 784개의 뉴런은 네트워크의 첫 번째 계층을 이루고 있습니다. 마지막 계층으로 넘어가면 10개의 뉴런이 있고, 각각은 하나의 숫자를 나타냅니다. 이 뉴런들의 활성화, 다시 말해 0과 1 사이의 수는 시스템이 특정 이미지가 특정 숫자와 일치하는 정도를 나타냅니다. 경험 상 네이밍되지 않은 계층에는 간단히 물음표로 남겨두는 것이 좋을 것입니다. 이 네트워크에서는 두 개의 감춰진 계층을 선택했습니다. 각각은 16개의 뉴런으로 구성되어 있고, 인정할 수 있도록 임의로 선택했습니다. 솔직히 말해서, 저는 이 구조를 동기부여하는 방식에 따라 두 개의 계층을 선택했고, 16은 화면 안에 들어가기 좋은 숫자였습니다. 구체적인 구조에 대해 실험할 여지가 많이 있습니다. 커넥션의 방식에 따라 각 계층의 활성화가 다음 계층의 활성화를 결정합니다. 네트워크의 핵심은 다음 계층에서의 활성화에 어떤 방식으로 녹아들게 하는지에 대한 것입니다. 이는 생물학적인 뉴런 네트워크의 경우에는 특정한 일부 그룹의 뉴런이 다른 특정 뉴런들이 활성화되도록 하는 것과 비슷하게 이해될 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pe-kWc5fU1rK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}